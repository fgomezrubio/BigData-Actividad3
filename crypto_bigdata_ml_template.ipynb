{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5849ee6",
   "metadata": {},
   "source": [
    "\n",
    "# 🧠 Aprendizaje Supervisado y No Supervisado con PySpark\n",
    "\n",
    "## 1. Introducción Teórica\n",
    "\n",
    "En esta sección se explican brevemente los conceptos clave de aprendizaje supervisado y no supervisado, junto con algunos de los algoritmos más representativos en PySpark.\n",
    "\n",
    "- **Aprendizaje Supervisado:** Técnicas donde el modelo se entrena usando un conjunto de datos etiquetado.\n",
    "  - Algoritmos: `DecisionTreeClassifier`, `RandomForestClassifier`, `GBTClassifier`, `MultilayerPerceptronClassifier`\n",
    "- **Aprendizaje No Supervisado:** Técnicas donde no existen etiquetas, el modelo encuentra patrones por sí solo.\n",
    "  - Algoritmos: `KMeans`, `GaussianMixture`, `PowerIterationClustering (PIC)`\n",
    "\n",
    "> Esta actividad se conecta con el análisis realizado en la entrega anterior, donde se caracterizó la población y se definieron reglas de particionamiento y muestreo estratificado.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abefaf59",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Selección de los Datos\n",
    "\n",
    "A continuación se cargará el dataset global unificado utilizando el script `crypto_data_merver_v2.py`, y se aplicarán las reglas de particionamiento y muestreo estratificado para construir la muestra M representativa.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d200292a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- datetime: timestamp (nullable = true)\n",
      " |-- open: double (nullable = true)\n",
      " |-- high: double (nullable = true)\n",
      " |-- low: double (nullable = true)\n",
      " |-- close: double (nullable = true)\n",
      " |-- volume: double (nullable = true)\n",
      " |-- coin: string (nullable = true)\n",
      " |-- frequency: string (nullable = true)\n",
      "\n",
      "+-------------------+--------+--------+--------+--------+--------+------+---------+\n",
      "|           datetime|    open|    high|     low|   close|  volume|  coin|frequency|\n",
      "+-------------------+--------+--------+--------+--------+--------+------+---------+\n",
      "|2017-07-14 00:00:00|    0.08|0.091033|    0.08|0.090993|1942.057|ETHBTC|       D1|\n",
      "|2017-07-15 00:00:00|0.090993|0.093699|0.087127|0.087635|4013.066|ETHBTC|       D1|\n",
      "|2017-07-16 00:00:00|0.087508|0.087635|0.075591|0.082241|8904.158|ETHBTC|       D1|\n",
      "|2017-07-17 00:00:00|0.082368|0.088394|0.081699|0.087537|6650.933|ETHBTC|       D1|\n",
      "|2017-07-18 00:00:00|0.087831|0.109068|0.084777|0.107732|7245.741|ETHBTC|       D1|\n",
      "+-------------------+--------+--------+--------+--------+--------+------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Inicializar Spark\n",
    "spark = SparkSession.builder     .appName(\"CryptoML\")     .getOrCreate()\n",
    "\n",
    "# Cargar dataset unificado directamente desde archivo CSV\n",
    "df = spark.read.csv(\"/Users/fgomezrubio/BigData/Evidencia1/dataset.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Mostrar esquema inicial\n",
    "df.printSchema()\n",
    "df.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b5e1115-175f-40cc-a5a8-9143afc3a942",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 6:======================================================>  (70 + 3) / 73]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+-----+\n",
      "|   coin|frequency|count|\n",
      "+-------+---------+-----+\n",
      "| ETHBTC|       M5|  600|\n",
      "|BTCUSDT|      M15| 2400|\n",
      "|ETHUSDT|      M30| 1400|\n",
      "|BNBUSDT|       H1| 5600|\n",
      "+-------+---------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "\n",
    "# Función para tomar una muestra fija por combinación coin-frequency\n",
    "def sample_partition(df: DataFrame, coin: str, frequency: str, sample_size: int) -> DataFrame:\n",
    "    return df.filter((df.coin == coin) & (df.frequency == frequency)).limit(sample_size)\n",
    "\n",
    "# Generar muestras por combinación coin-frequency\n",
    "ethbtc_m5 = sample_partition(df, \"ETHBTC\", \"M5\", 600)\n",
    "btcusdt_m15 = sample_partition(df, \"BTCUSDT\", \"M15\", 2400)\n",
    "ethusdt_m30 = sample_partition(df, \"ETHUSDT\", \"M30\", 1400)\n",
    "bnbusdt_h1 = sample_partition(df, \"BNBUSDT\", \"H1\", 5600)\n",
    "\n",
    "# Unión de todas las muestras para construir M\n",
    "df_M = ethbtc_m5.union(btcusdt_m15).union(ethusdt_m30).union(bnbusdt_h1)\n",
    "\n",
    "# Verificar la muestra resultante\n",
    "df_M.groupBy(\"coin\", \"frequency\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2e4606",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Preparación de los Datos\n",
    "\n",
    "Se realizará limpieza de nulos, conversión de tipos, normalización y ensamblado de vectores para modelos de ML.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d587d6cf-6f66-4707-a902-1171fddda276",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 36:======================================================> (71 + 2) / 73]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------------------------------------------------+-----+\n",
      "|features                                                                                             |label|\n",
      "+-----------------------------------------------------------------------------------------------------+-----+\n",
      "|[-0.5961914540357282,-0.5963768231890995,-0.5959901197032721,-0.5961972437121644,-0.6008288134550434]|0    |\n",
      "|[-0.5961914540357282,-0.5963768226436669,-0.5959901197032721,-0.5961972431640484,-0.6007997151929214]|1    |\n",
      "|[-0.5961914534876103,-0.596373332420813,-0.5959901191526252,-0.5961937357703635,-0.6007849273695157] |1    |\n",
      "|[-0.5961885550403977,-0.5963737578581979,-0.5959872959869363,-0.5961944329737964,-0.6002436841512904]|0    |\n",
      "|[-0.5961885632621655,-0.5963735505938309,-0.5959872155925164,-0.596193955016726,-0.6007749689028528] |1    |\n",
      "+-----------------------------------------------------------------------------------------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.sql.functions import when, col\n",
    "\n",
    "# 1. Eliminar registros con valores nulos\n",
    "df_clean = df_M.dropna()\n",
    "\n",
    "# 2. (Opcional) Convertir datetime a timestamp numérico si lo deseas usar como feature\n",
    "# df_clean = df_clean.withColumn(\"timestamp\", unix_timestamp(\"datetime\"))\n",
    "\n",
    "# 3. Crear columna binaria de clase: 1 si el precio sube, 0 si baja o se mantiene\n",
    "df_clean = df_clean.withColumn(\"label\", when(col(\"close\") > col(\"open\"), 1).otherwise(0))\n",
    "\n",
    "# 4. VectorAssembler para combinar columnas numéricas\n",
    "feature_cols = [\"open\", \"high\", \"low\", \"close\", \"volume\"]\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features_vec\")\n",
    "df_vector = assembler.transform(df_clean)\n",
    "\n",
    "# 5. Escalar los vectores de características\n",
    "scaler = StandardScaler(inputCol=\"features_vec\", outputCol=\"features\", withMean=True, withStd=True)\n",
    "scaler_model = scaler.fit(df_vector)\n",
    "df_scaled = scaler_model.transform(df_vector)\n",
    "\n",
    "# Mostrar las primeras filas\n",
    "df_scaled.select(\"features\", \"label\").show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfad9e42",
   "metadata": {},
   "source": [
    "\n",
    "## 4. Preparación del Conjunto de Entrenamiento y Prueba\n",
    "\n",
    "Se dividirá la muestra M en un conjunto de entrenamiento y uno de prueba. Se justificará el porcentaje utilizado.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b8f9790-6f5e-4ac1-9e26-d1944b761a5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño del conjunto de entrenamiento: 7100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 60:=======================================================>(72 + 1) / 73]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño del conjunto de prueba: 2900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# División 70% entrenamiento / 30% prueba\n",
    "train_data, test_data = df_scaled.randomSplit([0.7, 0.3], seed=42)\n",
    "\n",
    "# Verificar tamaños\n",
    "print(\"Tamaño del conjunto de entrenamiento:\", train_data.count())\n",
    "print(\"Tamaño del conjunto de prueba:\", test_data.count())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769e97f0",
   "metadata": {},
   "source": [
    "\n",
    "## 5. Construcción de Modelos\n",
    "\n",
    "### A. Modelo de Aprendizaje Supervisado\n",
    "\n",
    "Entrenamiento de un modelo como `RandomForestClassifier` con una variable objetivo derivada, por ejemplo: `label = close > open`.\n",
    "\n",
    "### B. Modelo de Aprendizaje No Supervisado\n",
    "\n",
    "Aplicación de `KMeans` o `PIC` sobre las características numéricas seleccionadas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd734b7c-eff3-4489-99bb-f8197171c6bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+----------------------------------------+\n",
      "|label|prediction|probability                             |\n",
      "+-----+----------+----------------------------------------+\n",
      "|1    |0.0       |[0.5267117169427934,0.47328828305720655]|\n",
      "|1    |0.0       |[0.5649099074168952,0.43509009258310477]|\n",
      "|1    |0.0       |[0.5649099074168952,0.43509009258310477]|\n",
      "|1    |0.0       |[0.5649099074168952,0.43509009258310477]|\n",
      "|0    |0.0       |[0.5639476704302548,0.43605232956974516]|\n",
      "|0    |0.0       |[0.5508931875154349,0.449106812484565]  |\n",
      "|1    |0.0       |[0.5639476704302548,0.43605232956974516]|\n",
      "|0    |0.0       |[0.5639476704302548,0.43605232956974516]|\n",
      "|0    |0.0       |[0.5267117169427934,0.47328828305720655]|\n",
      "|0    |0.0       |[0.5649099074168952,0.43509009258310477]|\n",
      "+-----+----------+----------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 257:=====================================================> (71 + 2) / 73]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precisión (Accuracy): 0.5837931034482758\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# ✅ Paso 5.A: Entrenamiento de modelo supervisado con RandomForestClassifier\n",
    "# Entrenaremos un modelo de clasificación binaria para predecir si el precio \n",
    "# de cierre es mayor al de apertura (label = 1).\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Crear modelo de Random Forest\n",
    "rf = RandomForestClassifier(featuresCol=\"features\", labelCol=\"label\", numTrees=100)\n",
    "\n",
    "# Entrenar el modelo\n",
    "rf_model = rf.fit(train_data)\n",
    "\n",
    "# Aplicar el modelo al conjunto de prueba\n",
    "predictions = rf_model.transform(test_data)\n",
    "\n",
    "# Mostrar algunas predicciones\n",
    "predictions.select(\"label\", \"prediction\", \"probability\").show(10, truncate=False)\n",
    "\n",
    "# Evaluar con métricas de clasificación\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Precisión (Accuracy):\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8170cac4-bd7b-480d-8a40-57bfefbd58d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                25/05/24 19:46:24 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "25/05/24 19:46:24 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.VectorBLAS\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette Score: 0.9342600563148852\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 378:=====================================================> (71 + 2) / 73]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------------------------------------------------+----------+\n",
      "|features                                                                                             |prediction|\n",
      "+-----------------------------------------------------------------------------------------------------+----------+\n",
      "|[-0.5961914540357282,-0.5963768231890995,-0.5959901197032721,-0.5961972437121644,-0.6008288134550434]|0         |\n",
      "|[-0.5961914540357282,-0.5963768226436669,-0.5959901197032721,-0.5961972431640484,-0.6007997151929214]|0         |\n",
      "|[-0.5961914534876103,-0.596373332420813,-0.5959901191526252,-0.5961937357703635,-0.6007849273695157] |0         |\n",
      "|[-0.5961885550403977,-0.5963737578581979,-0.5959872959869363,-0.5961944329737964,-0.6002436841512904]|0         |\n",
      "|[-0.5961885632621655,-0.5963735505938309,-0.5959872155925164,-0.596193955016726,-0.6007749689028528] |0         |\n",
      "|[-0.596188576416994,-0.5963736536805819,-0.5959872288080375,-0.5961940586106323,-0.6008142809769367] |0         |\n",
      "|[-0.5961882689228779,-0.5963735505938309,-0.5959869198952324,-0.596193955016726,-0.6006376708457517] |0         |\n",
      "|[-0.5961880666673897,-0.5963734524159728,-0.595986716706596,-0.5961938563558629,-0.6006344068667117] |0         |\n",
      "|[-0.5961880666673897,-0.5963733433294639,-0.595986716706596,-0.5961937829083315,-0.6006416009429631] |0         |\n",
      "|[-0.5961879932195973,-0.5963733596924402,-0.5959866429199366,-0.5961937631761587,-0.6006854759265212]|0         |\n",
      "+-----------------------------------------------------------------------------------------------------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "\n",
    "# Entrenar modelo de KMeans con 2 clusters (puedes probar con otros valores también)\n",
    "kmeans = KMeans(featuresCol=\"features\", k=2, seed=1)\n",
    "model = kmeans.fit(df_scaled)\n",
    "\n",
    "# Aplicar el modelo a todos los datos\n",
    "predictions_kmeans = model.transform(df_scaled)\n",
    "\n",
    "# Evaluar la calidad del agrupamiento\n",
    "evaluator = ClusteringEvaluator()\n",
    "\n",
    "silhouette = evaluator.evaluate(predictions_kmeans)\n",
    "print(\"Silhouette Score:\", silhouette)\n",
    "\n",
    "# Mostrar ejemplos de predicciones de cluster\n",
    "predictions_kmeans.select(\"features\", \"prediction\").show(10, truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60598e81-fdda-48ac-9c98-66e4938b6813",
   "metadata": {},
   "source": [
    "📊 Visualizar los clusters del modelo no supervisado (KMeans)\n",
    "Dado que estamos en PySpark y no se puede graficar directamente desde un DataFrame distribuido, primero convertiremos una muestra pequeña a Pandas para graficar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37465863-d8b8-4c2d-a98a-ca0e9c4c30cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 387:============>                                          (17 + 8) / 73]"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Convertir a Pandas una muestra de 1000 registros para visualización\n",
    "sample_pd = predictions_kmeans.select(\"features\", \"prediction\").limit(1000).toPandas()\n",
    "\n",
    "# Extraer valores del vector 'features'\n",
    "sample_pd[\"features\"] = sample_pd[\"features\"].apply(lambda x: x.toArray())\n",
    "features_array = pd.DataFrame(sample_pd[\"features\"].tolist())\n",
    "\n",
    "# Agregar la predicción como columna\n",
    "features_array[\"cluster\"] = sample_pd[\"prediction\"]\n",
    "\n",
    "# Visualizar los dos primeros componentes (open vs high escalados)\n",
    "plt.figure(figsize=(10, 6))\n",
    "for cluster in features_array[\"cluster\"].unique():\n",
    "    subset = features_array[features_array[\"cluster\"] == cluster]\n",
    "    plt.scatter(subset[0], subset[1], label=f\"Cluster {cluster}\", alpha=0.6)\n",
    "\n",
    "plt.title(\"Visualización de Clusters con KMeans\")\n",
    "plt.xlabel(\"Componente 1 (scaled open)\")\n",
    "plt.ylabel(\"Componente 2 (scaled high)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373e93eb-380a-4baa-8f45-9e45333e891c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-pyspark",
   "language": "python",
   "name": "env-pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
